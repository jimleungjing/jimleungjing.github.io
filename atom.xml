<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Jim&#39;s Starship</title>
  
  <subtitle>To boldly go where no one has gone before.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jimleungjing.github.io/"/>
  <updated>2018-12-10T12:54:16.185Z</updated>
  <id>http://jimleungjing.github.io/</id>
  
  <author>
    <name>Jim Leung</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>The Issues on Training RNN</title>
    <link href="http://jimleungjing.github.io/2018/11/20/On-the-Difficulty-of-Training-RNN/"/>
    <id>http://jimleungjing.github.io/2018/11/20/On-the-Difficulty-of-Training-RNN/</id>
    <published>2018-11-20T10:12:01.000Z</published>
    <updated>2018-12-10T12:54:16.185Z</updated>
    
    <content type="html"><![CDATA[<!--1.       Problem: the problem to be solved, proposed by the author2.       Solution: how the author solves the proposed problem3.       Novelty: the difference from previous related work, and pick out the most related paper4.       Take-away: what you learn from this paper and want to remember--><p>&emsp;In this post, we are going to briefly discuss the issues of training recurrent neural network(RNN)—the exploding and vanishing gradient problems. To learn further about the mechanism, we recommend reading the paper <a href="https://arxiv.org/pdf/1211.5063.pdf" target="_blank" rel="noopener">On the difficulty of training recurrent neural networks, R. Pascanu et al. (2012)</a> or <a href="http://ai.dinfo.unifi.it/paolo//ps/tnn-94-gradient.pdf" target="_blank" rel="noopener">Learning Long-Term Dependencies with Gradient Descent is Difficult, Bengio, et al. (1994)</a> for a formal and detailed theory. And this post is mainly based on the former paper.</p><h2 id="Exploding-and-Vanishing-Gradient"><a href="#Exploding-and-Vanishing-Gradient" class="headerlink" title="Exploding and Vanishing Gradient"></a>Exploding and Vanishing Gradient</h2><h3 id="RNN-Model"><a href="#RNN-Model" class="headerlink" title="RNN Model"></a>RNN Model</h3><p>&emsp;Using the notations in my another post: <a href="">The Computation of Gradients on RNN by BPTT</a>, also referred to the <a href="http://www.deeplearningbook.org" target="_blank" rel="noopener"><em>Deep Learning</em></a>, a RNN model with input $\boldsymbol{x}$ and hidden units $\boldsymbol{h}$ at time step $t$ is given by:</p><script type="math/tex; mode=display">\begin{align*}\boldsymbol{h}^{(t)} &= \sigma(\boldsymbol{Ux}^{(t)}+\boldsymbol{Wh}^{(t-1)}+\boldsymbol{b}) \tag{1}\end{align*}</script><p>where the parameters $\boldsymbol{U}$, $\boldsymbol{W}$, $\boldsymbol{b}$ represent the input-to-hidden connections,hidden-to-hidden connections and bias vector respectively, and $\sigma$ is denoted as an activation function.</p><h3 id="Mechanism"><a href="#Mechanism" class="headerlink" title="Mechanism"></a>Mechanism</h3><p>&emsp;As a convenience, we here will take the weight matrix $\boldsymbol{W}$ for example to analyze the exploding and vanishing gradients problems. Introduced in <a href="http://www.deeplearningbook.org" target="_blank" rel="noopener"><em>Deep Learning</em></a>, variable $\boldsymbol{W}^{(t)}$, which is defined to be the copy of weight matrix $\boldsymbol{W}$ used only in time $t$, will be used in the computation of $\frac{\partial{L}}{\partial{\boldsymbol{W}}}$ by accumulating the factor $\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} $ through time. The computation of gradient $\frac{\partial{L}}{\partial{\boldsymbol{W}}}$ can be written in a sum-of-products form as follows:</p><script type="math/tex; mode=display">\begin{align*}\frac{\partial{L}}{\partial{\boldsymbol{W}}} &=\sum_{1 \le t \le \tau}\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} \tag{2}\\\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} &=\sum_{t \le k \le \tau}\frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}}\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}}\frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}} \tag{3} \\\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}} &=\prod_{t \lt i \le k}\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}} = \prod_{t \lt i \le k}\boldsymbol{W}^{\mathsf{T}}\text{diag}(\sigma'(\boldsymbol{h}^{(i-1)})) \tag{4}\end{align*}</script><p>where $\sigma’$ computes element-wise the derivative of $\sigma$.<br>As the equation (3) shows, gradient term $\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} $ consists of temporal components $\frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}}<br>\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}}\frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}}$ that measures how $L$ at setp $k$ affects the gradient at step $t \lt k$. Notice that factor $\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}}$ take the form of a product of $k-t$ Jacobian matrices (see eq. (4)), naturally, we’d think it behaves in the same way a product of $k-t$ real number $n$ shrink to zero ($0\lt n \lt 1$) or explode to infinity ($n \gt 1$) when $k-t$ approaches to infinity.<br>To formalize these intuitions, in what follows a method proposed by <a href="https://arxiv.org/pdf/1211.5063.pdf" target="_blank" rel="noopener">R. Pascanu et al. (2012)</a> will be used to obtain tight conditions for when the gradients explode or vanish.<br>At first, let’s set $\sigma$ to the identity function in equation (1) so that the $\text{diag}(\sigma’(\boldsymbol{h}^{(i-1)}))$ will be an identity matrix $\boldsymbol{I}$. Consequently, equtaion (4) could be written in a product of $k-t$ matrices form:</p><script type="math/tex; mode=display">\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}} = \prod_{t \lt i \le k}\boldsymbol{W}^{\mathsf{T}} = \left(\boldsymbol{W}^{\mathsf{T}}\right)^{(k-t)}   \tag{5}</script><p>, and $\boldsymbol{W}$ can be further diagonalized to better understand the $k-t$th power of $\boldsymbol{W}$ if the n by n matrix $\boldsymbol{W}$ has n linearly indepentent eigenvectors:</p><script type="math/tex; mode=display">\begin{align*}\boldsymbol{W} &= \boldsymbol{S}\boldsymbol{\Lambda}\boldsymbol{S}^{-1} \tag{6}\\\left(\boldsymbol{W}^{\mathsf{T}}\right)^{(k-t)} &= \left(\boldsymbol{S}\boldsymbol{\Lambda}^{(k-t)}\boldsymbol{S}^{-1}\right)^{\mathsf{T}}    \tag{7}\end{align*}</script><p>where $\boldsymbol{\Lambda}$ is the eigenvalue matrix and $\boldsymbol{S}$ is eigenvector matrix. Induced by eq. (7), it’s sufficient for $0 \lt \rho \lt 1$, where the $\rho$ is the spectral radius of the weight matrix $\boldsymbol{W}$, for long term components to vanish (as $t \to \infty$) and necessary for $\rho \gt 1$ for them to blow up.<br>Since the norm is used for matrices to measure the size of a matrix, we  now generalize the reasult above for nonlinear functions $\sigma$ where $|\sigma’(x)|$ is bounded, $\Vert\text{diag}(\sigma’(\boldsymbol{x}))\Vert \le \gamma \in \mathcal{R}$, by relying on singular values.<br>We suppose that it is sufficient for $\lambda_1 \lt \frac{1}{\gamma}$, where $\lambda_1$ is the largest singular value of $\boldsymbol{W}$, for the vanishing gradient problem. Note that Jocabian matrix $\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}}$ is given by $\boldsymbol{W}^{\mathsf{T}}\text{diag}(\sigma’(\boldsymbol{h}^{(i-1)}))$, we apply the triangle inequality of 2-norm of matrix to it:</p><script type="math/tex; mode=display">\forall i,\ \left\Vert\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}}\right\Vert \le \left\Vert\boldsymbol{W}^{\mathsf{T}}\right\Vert \left\Vert\text{diag}(\sigma'(\boldsymbol{h}^{(i-1)})) \right\Vert\lt\frac{1}{\gamma}\gamma = 1  \tag{8}</script><p>Let $\eta \in \mathbb{R}$ be such that $\forall i, \left\Vert\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}} \right\Vert \le \eta \lt 1$, where the existence of $\eta$ is given by eq. (8). With this supremum, we have:</p><script type="math/tex; mode=display">\left\Vert\frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}}\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}}\frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}}\right\Vert \le\eta^{k-t}\left\Vert\frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}}\right\Vert\left\Vert\frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}}\right\Vert \tag{9}</script><p>As the equation above shown, the long term contribution will go to 0 exponentially fast as $t-k$ grows because of $\eta \lt 1$. Similarly, we can get the necessary condition for the largest singular value $\lambda_1 \gt \frac{1}{\gamma}$, for the exploding gradient problem. For activation function sigmoid we have $\gamma = \frac{1}{4}$, for tanh we have $\gamma = 1$.</p><h2 id="Solutions-to-the-vanishing-and-exploding-gradient"><a href="#Solutions-to-the-vanishing-and-exploding-gradient" class="headerlink" title="Solutions to the vanishing and exploding gradient"></a>Solutions to the vanishing and exploding gradient</h2><h3 id="Scaling-down-the-exploding-gradient"><a href="#Scaling-down-the-exploding-gradient" class="headerlink" title="Scaling down the exploding gradient"></a>Scaling down the exploding gradient</h3><p>Using L1 or L2 penalty on recurrent weight matrix is a good choice to deal with the exploding gradient problem. Compared with it, clipping the norm of matrix would be more simple and computationally efficient whenever it goes over a threshold although it introduce an additional hyper-parameter. </p><h3 id="Vanishing-gradient-regularization"><a href="#Vanishing-gradient-regularization" class="headerlink" title="Vanishing gradient regularization"></a>Vanishing gradient regularization</h3><p>To address the vanishing gradient problem, <a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735" target="_blank" rel="noopener">Hochreiter and Schmidhuber (1997)</a> propose the LSTM model that enforcing constant error flow through some special units. And <a href="https://arxiv.org/pdf/1211.5063.pdf" target="_blank" rel="noopener">R. Pascanu et al. (2012)</a> presented a regularization term so that gradients neither increase or decrease in magnitude. But frankly speaking, I can’t catch the point what the regularization term excatly means in mathematics.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this post, I simply analyze the exploding and vanishing gradient problems in training RNNs by exploring the norm of gradients on weight matrices due to <a href="https://arxiv.org/pdf/1211.5063.pdf" target="_blank" rel="noopener">R. Pascanu et al. (2012)</a>. The problems seem to be an obstacle keeping people from retaining information from the long time lags. Hopefully, many approaches were suggested to deal with them. For instance, it is universally acknowledged that LSTM models are widely applied in a varity of tasks and lead to the incredible successes. So, I’d like to explore the LSTMs in next post.   </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!--
1.       Problem: the problem to be solved, proposed by the author

2.       Solution: how the author solves the proposed problem

3.  
      
    
    </summary>
    
      <category term="Paper Notes" scheme="http://jimleungjing.github.io/categories/Paper-Notes/"/>
    
    
      <category term="RNN" scheme="http://jimleungjing.github.io/tags/RNN/"/>
    
  </entry>
  
  <entry>
    <title>The Computation of Gradients on RNN by BPTT</title>
    <link href="http://jimleungjing.github.io/2018/11/08/BPTT/"/>
    <id>http://jimleungjing.github.io/2018/11/08/BPTT/</id>
    <published>2018-11-08T08:53:42.000Z</published>
    <updated>2018-11-25T13:59:28.352Z</updated>
    
    <content type="html"><![CDATA[<p>&emsp;As a beginner of deep learning, it’s quite easy to get lost when you attempt to compute the gradient on a recurrent neural network(RNN). Therefore, we will give the computation of gradients on RNN by backforward propagation through time(BPTT). I assume that you have known the concept of backward-propagation algorithm, <a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" target="_blank" rel="noopener">recurrent neural networks</a>(RNN), <a href="http://math.mit.edu/~gs/linearalgebra/" target="_blank" rel="noopener">linear algebra</a> and <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">matrix calculus</a> before the exploration.</p><h2 id="Notations"><a href="#Notations" class="headerlink" title="Notations"></a>Notations</h2><p>&emsp;These notations used in the following computation refer to the book <a href="http://www.deeplearningbook.org" target="_blank" rel="noopener"><em>Deep Learning</em></a>.<br>$<br>\begin{array}{l|l}\hline<br>a &amp; \text{Scalar a}\\\hline<br>\boldsymbol{x} &amp; \text{Vector x} \\\hline<br>\boldsymbol{1}_{n} &amp; \text{A vector [1,1,…,1] with n column}\\\hline<br>\boldsymbol{A} &amp; \text{Matrix A}\\\hline<br>\boldsymbol{A}^{-1} &amp; \text{The inverse matrix of matrix A}\\\hline<br>\boldsymbol{A}^{\mathsf{T}} &amp; \text{Transpose of matrix A}\\\hline<br>\text{diag}(\boldsymbol{a}) &amp; \text{Diagnal matrix with diagnal entries gieven by }\boldsymbol{a}\\\hline<br>\boldsymbol{I} &amp; \text{Identity matrix}\\\hline<br>\nabla_{\boldsymbol{x}}y &amp; \text{Gradient }y\text{ with respect to }\boldsymbol{x} \\\hline<br>\frac{\partial{y}}{\partial{x}} &amp; \text{Partial derivative of } y \text{ with respect to }x\\\hline<br>\frac{\partial{\boldsymbol{y}}}{\partial{\boldsymbol{x}}} &amp; \text{Jacobian matrix }\boldsymbol{J} \in \mathbb{R}^{m \times n} \text{of } f:\mathbb{R}^{n}\to\mathbb{R}^{m} \\\hline<br>\end{array}<br>$</p><h2 id="Review-of-RNN"><a href="#Review-of-RNN" class="headerlink" title="Review of RNN"></a>Review of RNN</h2><!--在align环境中根据'&'对齐文本--><p><center><img src="/2018/11/08/BPTT/RNN.png" alt="The computational graph of RNN" width="70%" length="70%"></center></p><p></p><p style="text-align:center">Figure 2.1: The computational graph of RNN @<i><a href="http://www.deeplearningbook.org" target="_blank" rel="noopener">Deep Leaerning</a></i></p><br>&emsp;Let’s develop the forward propagation equations for the RNN dipcted in Figure 2.1. As a convenience, we use right superscription to indiacte the specified state of RNN at some time $ t $.<br>To represent the hidden units of the network, we use the variable $ \boldsymbol{h} $, which will be the input of hyperbolic tangent activation function. As you see, the RNN has input-to-hidden connections parametrized by a weight matrix $ \boldsymbol{U} $, hidden-to-hidden connections parametrized by a weight matrix $ \boldsymbol{W} $, hidden-to-output connections parametrized by a weight matrix $ \boldsymbol{V} $. We can apply the softmax operation to the output $ \boldsymbol{o} $ to obtain a vector $ \boldsymbol{\hat{y}} $ of normalized probabilities over the output. We define the loss function as the negative log-likelihood of output vector $ \boldsymbol{\hat{y}} $ given the training target $ \boldsymbol{y} $. With notations and definitions made, we have the following equations:<p></p><script type="math/tex; mode=display">\left\{\begin{aligned}\boldsymbol{a}^{(t)} &= \boldsymbol{b} + \boldsymbol{Wh}^{(t-1)} + \boldsymbol{Ux}^{(t)} &\qquad (2.1)\\\boldsymbol{h}^{(t)} &= \text{tanh}(\boldsymbol{a}^{(t)}) &\qquad (2.2)\\\boldsymbol{o}^{(t)} &= \boldsymbol{c} + \boldsymbol{Vh}^{(t)} &\qquad (2.3)\\\boldsymbol{\hat{y}}^{(t)} &= \text{softmax}(\boldsymbol{o}^{(t)}) &\qquad (2.4)\\L^{(t)} &= -{\boldsymbol{y}^{(t)}}^{\mathsf{T}}\text{log}(\boldsymbol{\hat{y}}^{(t)}) &\qquad (2.5)\end{aligned}\right.</script><p>where both $ \text{tanh} $ and $\text{log}$ are element-wise function.</p><h2 id="Taking-the-Derivatives"><a href="#Taking-the-Derivatives" class="headerlink" title="Taking the Derivatives"></a>Taking the Derivatives</h2><p>&emsp;Known the notations and concepts above, we can compute the gradients by BPTT using the <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Identities" target="_blank" rel="noopener">identities</a> below.</p><div class="table-container"><table><thead><tr><th style="text-align:center">Conditions</th><th style="text-align:center">Expression</th><th style="text-align:center">Denominator layout</th></tr></thead><tbody><tr><td style="text-align:center"><script type="math/tex">a = a(\boldsymbol{x}), \boldsymbol{u = u}(\boldsymbol{x})</script></td><td style="text-align:center"><script type="math/tex">\frac{\partial{a\boldsymbol{u}}}{\partial{\boldsymbol{x}}}</script></td><td style="text-align:center"><script type="math/tex">a\frac{\partial{\boldsymbol{u}}}{\partial{\boldsymbol{x}}} + \frac{\partial{a}}{\partial{\boldsymbol{x}}}\boldsymbol{u}^{\mathsf{T}}</script></td></tr><tr><td style="text-align:center">$ \boldsymbol{A} $ is not a function of $ \boldsymbol{x} $</td><td style="text-align:center">$ \frac{\partial{\boldsymbol{Ax}}}{\partial{\boldsymbol{x}}} $</td><td style="text-align:center">$ \boldsymbol{A}^{\mathsf{T}} $</td></tr><tr><td style="text-align:center">$g(\boldsymbol{U})\text{ is a scalar, }\boldsymbol{U} = \boldsymbol{U}(\boldsymbol{X}) $</td><td style="text-align:center">$\frac{\partial{g(\boldsymbol{U})}}{\partial{\boldsymbol{X}}_{ij}} $</td><td style="text-align:center">$\text{tr}\left(\left(\frac{\partial{g(\boldsymbol{U})}}{\partial{\boldsymbol{U}}}\right)^{\mathsf{T}}\frac{\partial{\boldsymbol{U}}}{\partial{\boldsymbol{X}}_{ij}}\right) $</td></tr></tbody></table></div><p>we here will give three examples of computations in detail.</p><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h3><p>To begin with, we will take the derivative of loss function $ L $ with respect to vector $ \boldsymbol{o} $. Using <a href="https://en.wikipedia.org/wiki/Matrix_calculus#Denominator-layout_notation" target="_blank" rel="noopener">denominator-layout</a> and chain rule, we have:</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{o}^{(t)}}L &= \left(\frac{\partial{\boldsymbol{\hat{y}}}}{\partial{\boldsymbol{o}^{(t)}}}\frac{\partial{L}}{\partial{\boldsymbol{\hat{y}}}}\right)\end{aligned}  \tag{3.1.1}</script><p>Taking the former derivative of $ \boldsymbol{\hat{y}} $ with respect to $ \boldsymbol{o} $, we get:</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial{\boldsymbol{\hat{y}}}}{\partial{\boldsymbol{o}^{(t)}}}&= \frac{\partial{\frac{\text{exp}(\boldsymbol{o^{(t)}})}{\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}}}{\partial{\boldsymbol{o}^{(t)}}}\\&= \frac{1}{\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}\frac{\partial{\text{exp}(\boldsymbol{o}^{(t)})}}{\partial{\boldsymbol{o}^{(t)}}}+\frac{\partial{\frac            {1}            {\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}}}{\partial{\boldsymbol{o}^{(t)}}}\text{exp}(\boldsymbol{o}^{(t)})^{\mathsf{T}}\\&=\frac{1}{\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}\text{diag}\left(    \text{exp}(\boldsymbol{o}^{(t)})    \right)-\left({\frac    {1}    {\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}}\right)^2\text{exp}(\boldsymbol{o}^{(t)})\text{exp}(\boldsymbol{o}^{(t)})^{\mathsf{T}}\\&=\text{diag}(\boldsymbol{\hat{y}}) - \boldsymbol{\hat{y}}{\boldsymbol{\hat{y}}}^{\mathsf{T}} \end{aligned} \tag{3.1.2}</script><p>For the latter partial derivative, we have:</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial{L}}{\partial{\boldsymbol{\hat{y}}}}&=\frac{\partial{-\boldsymbol{y}^{\mathsf{T}}\text{log}(\boldsymbol{\hat{y}})}}{\partial{\boldsymbol{\hat{y}}}}\\&=\frac{\partial{\text{log}(\boldsymbol{\hat{y}})}}{\partial{\boldsymbol{\hat{y}}}}\frac{\partial{-\boldsymbol{y}^{\mathsf{T}}\text{log}(\boldsymbol{\hat{y}})}}{\partial{\text{log}(\boldsymbol{\hat{y}})}}\\&=\text{diag}(\boldsymbol{\hat{y}})^{-1}(-\boldsymbol{y})\end{aligned} \tag{3.1.3}</script><p>Therefore, gradient $ \nabla_{\boldsymbol{o}^{(t)}}L $ at time step $ t $ is as follows:</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{o}^{(t)}}L &= \frac{\partial{\boldsymbol{\hat{y}}}}{\partial{\boldsymbol{o}^{(t)}}}\frac{\partial{L}}{\partial{\boldsymbol{\hat{y}}}}\\&=    \left(    \text{diag}(\boldsymbol{\hat{y}}) - \boldsymbol{\hat{y}}\boldsymbol{\hat{y}}^{\mathsf{T}}    \right)\left(    \text{diag}(\boldsymbol{\hat{y}})^{-1}(-\boldsymbol{y})    \right)\\&=\left(\boldsymbol{I-\hat{y}}\boldsymbol{1_c}^{\mathsf{T}}\right)(-\boldsymbol{y})\\&=\boldsymbol{\hat{y}-y}\end{aligned} \tag{3.1.4}</script><p>where the training target $ \boldsymbol{y} $ is a basic vector [0,…,0,1,0,…,0] with a 1 at the postion $ i $.</p><h3 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2"></a>Example 2</h3><p>&emsp;Now, we consider a slightly more complicate example: computation of $\nabla_{\boldsymbol{h}^{(t)}}L$.<br>Walking through computational graph backforward, it’s easy to get the gradient at the final time step $ \tau $ because the $\boldsymbol{h}^{(t)}$ has only a descendent $\boldsymbol{o}^{(t)}$: </p><script type="math/tex; mode=display">\nabla_{\boldsymbol{h}^{(t)}}L = \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L \tag{3.2.1}</script><p>However, we should note that the $\boldsymbol{h}^{(t)}$ has both $\boldsymbol{o}^{(t)}$ and $\boldsymbol{h}^{(t+1)}$ as descendents from $ t = \tau - 1 $ down to $t = 1$. Consequently, the gradient is given by:</p><script type="math/tex; mode=display">\nabla_{\boldsymbol{h}^{(t)}}L = \left(    \frac{\partial{\boldsymbol{h}^{(t+1)}}}{\partial{\boldsymbol{h}^{(t)}}}\right)^{\mathsf{T}}\nabla_{\boldsymbol{h}^{(t+1)}}L+\boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L \tag{3.2.2}</script><p>Taking the two cases above into account, we have:</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{h}^{(t)}}L &= \begin{cases}\boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad \text{if t at the final time step }\tau \\\boldsymbol{W}^{\mathsf{T}}\text{diag}\left(    1-(\boldsymbol{h}^{(t+1)})^2    \right)    (\nabla_{\boldsymbol{h}^{(t+1)}}L)+\boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad 1\le t \le \tau - 1\\\end{cases}\\\end{aligned} \tag{3.2.3}</script><h3 id="Example-3"><a href="#Example-3" class="headerlink" title="Example 3"></a>Example 3</h3><p>&emsp;Once the gradients on internal nodes were computed, we can take the derivatives of the parameters, e.g. $ \boldsymbol{W} $, $ \boldsymbol{V} $. Notice that the parameters are shared across the time steps, we will introduce some dummy variables such as $\boldsymbol{W}^{(t)}$ that are define to be copies of parameter but with each only used at time step $t$, which will be accumulated from $ t = \tau $ down to $ t = 0 $ so that we can obtain the gradient $ \nabla_{\boldsymbol{W}}L $ at the end of a backward propagation.<br>Applying the notations above, the gradient on the parameter $\nabla_{\boldsymbol{V}}L $ is given by:</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{V}}L &= \sum_{t = 0}^{\tau}\nabla_{\boldsymbol{V}^{(t)}}L\\&= \sum_{t = 0}^{\tau}\left[\begin{matrix}\text{tr}\left((\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{11}}}\right) &\cdots & \text{tr}\left((\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{1n}}}\right)\\ \vdots &\ddots & \vdots\\\text{tr}\left((\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{n1}}}\right) &\cdots & \text{tr}\left((\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{nn}}}\right) \end{matrix}\right]\\&=\sum_{t = 0}^{\tau}\left[\begin{matrix}(\nabla_{\boldsymbol{o}^{(t)}}L)_{1}\boldsymbol{h}^{(t)}_{1} & \cdots & (\nabla_{\boldsymbol{o}^{(t)}}L)_{1}\boldsymbol{h}^{(t)}_{n}\\\vdots &\ddots & \vdots\\(\nabla_{\boldsymbol{o}^{(t)}}L)_{n}\boldsymbol{h}^{(t)}_{1} & \cdots & (\nabla_{\boldsymbol{o}^{(t)}}L)_{n}\boldsymbol{h}^{(t)}_{n}\end{matrix}\right]\\&=\sum_{t = 0}^{\tau}(\nabla_{\boldsymbol{o}^{(t)}}L){\boldsymbol{h}^{(t)}}^{\mathsf{T}}\end{aligned} \tag{3.3.1}</script><p>In fact, inspired by the relationship between total derivative and partial derivative in multivariable calculus:</p><script type="math/tex; mode=display">\text{d}f = \sum_{i=1}^{n}\frac{\partial{f}}{\partial{x_i}}\text{d}x_i \tag{3.3.2}</script><p>which can be written in vector form as:</p><script type="math/tex; mode=display">\text{d}f = \left(\frac{\partial{f}}{\partial{\boldsymbol{x}}}\right)^{\mathsf{T}}\text{d}\boldsymbol{x} \tag{3.3.3}</script><p>we can obtian the relationship between total derivative and partial derivative in matrix calcalus:</p><script type="math/tex; mode=display">\begin{aligned}\text{d}f &= \sum_{i=1}^{m}\sum_{j=1}^{n}\frac{\partial{f}}{\partial{\boldsymbol{X}_{ij}}}\text{d}\boldsymbol{X}_{ij}\\&=\text{tr}\left(\left(    \frac{\partial{f}}{\partial{\boldsymbol{X}}}\right)^{\mathsf{T}}\text{d}\boldsymbol{X}\right)\end{aligned} \tag{3.3.4}</script><p>Now, we can skip the tedious process (3.3.1) by applying eq. (3.3.4) for gradient $ \nabla_{\boldsymbol{V}}L $:</p><script type="math/tex; mode=display">\begin{aligned}\text{d}L &= \text{tr}\left(\left(\frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}\right)^{\mathsf{T}}\text{d}\boldsymbol{o}^{(t)}\right)\\&=\text{tr}\left(\left(    \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}\right)^{\mathsf{T}}\text{d}\left(\boldsymbol{c} + \boldsymbol{V}^{(t)}\boldsymbol{h}^{(t)}\right)\right)\\&=\text{tr}\left(\left(    \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}\right)^{\mathsf{T}}\left(\left(\text{d}\boldsymbol{V}^{(t)}\right)\boldsymbol{h}^{(t)}+\boldsymbol{V}^{(t)}\text{d}\boldsymbol{h}^{(t)}\right)\right)\\&=\text{tr}\left(\left(    \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}\right)^{\mathsf{T}}\left(\text{d}\boldsymbol{V}^{(t)}\right)\boldsymbol{h}^{(t)}\right)\\&=\text{tr}\left(    \boldsymbol{h}^{(t)}\left(    \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}\right)^{\mathsf{T}}\left(\text{d}\boldsymbol{V}^{(t)}\right)\right) \end{aligned} \tag{3.3.5}</script><p>We could derive the result by comparing eq. (3.3.4) with eq. (3.3.5):</p><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{V}^{(t)}}L &= \frac{\partial{L}}{\partial{\boldsymbol{V}^{(t)}}}\\&=\left(\boldsymbol{h}^{(t)}\left(    \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}\right)^{\mathsf{T}}\right)^{\mathsf{T}}\\&=\frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}{\boldsymbol{h}^{(t)}}^{\mathsf{T}}\\&=\left(\nabla_{\boldsymbol{o}^{(t)}}L\right){\boldsymbol{h}^{(t)}}^{\mathsf{T}}\end{aligned}   \tag{3.3.6}</script><script type="math/tex; mode=display">\begin{aligned}\nabla_{\boldsymbol{V}}L &= \sum_{i=0}^{\tau}\nabla_{\boldsymbol{V}^{(t)}}L\\&=\sum_{i=0}^{\tau}\left(\nabla_{\boldsymbol{o}^{(t)}}L\right){\boldsymbol{h}^{(t)}}^{\mathsf{T}}\end{aligned}   \tag{3.3.7}</script><p>Similarly, using the equation (3.3.4), the gradient on remaining parameters is quite easy to get and we can gather these gradients as follows:</p><script type="math/tex; mode=display">\begin{align*}\nabla_{\boldsymbol{o}^{(t)}}L &= \boldsymbol{\hat{y}-y} \tag{3.3.8} \\\nabla_{\boldsymbol{h}^{(t)}}L &= \begin{cases}\boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad \text{if t at the final time step }\tau \\\boldsymbol{W}^{\mathsf{T}}\text{diag}\left(    1-(\boldsymbol{h}^{(t+1)})^2    \right)    (\nabla_{\boldsymbol{h}^{(t+1)}}L)+\boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad 1\le t \le \tau - 1\\\end{cases} \tag{3.3.9} \\\nabla_{\boldsymbol{c}}L &= \sum_{t}\nabla_{\boldsymbol{o}^{(t)}}L \tag{3.3.10} \\\nabla_{\boldsymbol{b}}L &= \sum_{t}\text{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\nabla_{\boldsymbol{h}^{(t)}}L \tag{3.3.11} \\\nabla_{\boldsymbol{V}}L &= \sum_{t}\left(\nabla_{\boldsymbol{o}^{(t)}}L\right){\boldsymbol{h}^{(t)}}^{\mathsf{T}} \tag{3.3.12} \\\nabla_{\boldsymbol{W}}L &= \sum_{t}\text{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t)}}L\right){\boldsymbol{h}^{(t-1)}}^{\mathsf{T}} \tag{3.3.13} \\\nabla_{\boldsymbol{U}}L &= \sum_{t}\text{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t)}}L\right){\boldsymbol{x}^{(t)}}^{\mathsf{T}} \tag{3.3.14}\end{align*}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&amp;emsp;As a beginner of deep learning, it’s quite easy to get lost when you attempt to compute the gradient on a recurrent neural network(
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://jimleungjing.github.io/categories/Deep-Learning/"/>
    
    
      <category term="RNN" scheme="http://jimleungjing.github.io/tags/RNN/"/>
    
      <category term="BPTT" scheme="http://jimleungjing.github.io/tags/BPTT/"/>
    
  </entry>
  
  <entry>
    <title>An Overview to LSTM</title>
    <link href="http://jimleungjing.github.io/2018/10/23/LSTM/"/>
    <id>http://jimleungjing.github.io/2018/10/23/LSTM/</id>
    <published>2018-10-23T15:53:42.000Z</published>
    <updated>2018-11-20T10:12:44.756Z</updated>
    
    <content type="html"><![CDATA[<!--1.       Problem: the problem to be solved, proposed by the author2.       Solution: how the author solves the proposed problem3.       Novelty: the difference from previous related work, and pick out the most related paper4.       Take-away: what you learn from this paper and want to remember--><h2 id="The-Problem-LSTM-Addressed"><a href="#The-Problem-LSTM-Addressed" class="headerlink" title="The Problem LSTM Addressed"></a>The Problem LSTM Addressed</h2><p>&emsp;In the past decades, recurrent neural networks (RNNs) have been successufully applied to a variety problems: speech recognition, language modeling, prediction, etc. RNNs can use their internal state to process sequences of inputs and persist information as shown below in Fig. 1.<br>&emsp;However, the RNNs don’t seem to be able to learn of long term dependencies in the input/output sequences. It’s believed the reason for that problem is that error signals flowing backward in time tend to blow up or vanish(<a href="http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf" target="_blank" rel="noopener">Hochreiter 1991</a>).</p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p>&emsp;Fortunately, LSTM address this<br>Learning to store information </p><h2 id="RNN-amp-IIR"><a href="#RNN-amp-IIR" class="headerlink" title="RNN &amp; IIR"></a>RNN &amp; IIR</h2><p>　　The similarity between RNN(Recurrent Neural Network) and IIR(Infinite Inpulse Response) suddenly hit me when I learned the concept about LSTM at first.<br>  I realized that it is similar between RNN and IIR when I first came to the RNN.In the following study, the idea hits me again when learning Adam gredient descent. As a user of Reddit said, it is a two FIR.<br>  I guess that I’m not the first person to conceive the idea of similarity between RNN and IIR. So I Google the key words “IIR LSTM”. Not surprisingly, there are several papers based on the similarity between the IIR and RNN appear in the search results list. In the most recent paper “Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency”, the authors proposed a novel network structure to model sequential signals like speech or language. </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;!--
1.       Problem: the problem to be solved, proposed by the author

2.       Solution: how the author solves the proposed problem

3.  
      
    
    </summary>
    
      <category term="Paper Notes" scheme="http://jimleungjing.github.io/categories/Paper-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://jimleungjing.github.io/tags/Machine-Learning/"/>
    
      <category term="Deep Learning" scheme="http://jimleungjing.github.io/tags/Deep-Learning/"/>
    
      <category term="LSTM" scheme="http://jimleungjing.github.io/tags/LSTM/"/>
    
  </entry>
  
</feed>
