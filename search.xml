<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[The Issues on Training RNN]]></title>
    <url>%2F2018%2F11%2F20%2FOn-the-Difficulty-of-Training-RNN%2F</url>
    <content type="text"><![CDATA[&emsp;In this post, we are going to briefly discuss the issues of training recurrent neural network(RNN)—the exploding and vanishing gradient problems. To learn further about the mechanism, we recommend reading the paper On the difficulty of training recurrent neural networks, R. Pascanu et al. (2012) or Learning Long-Term Dependencies with Gradient Descent is Difficult, Bengio, et al. (1994) for a formal and detailed theory. And this post is mainly based on the former paper. Exploding and Vanishing GradientRNN Model&emsp;Using the notations in my another post: The Computation of Gradients on RNN by BPTT, also referred to the Deep Learning, a RNN model with input $\boldsymbol{x}$ and hidden units $\boldsymbol{h}$ at time step $t$ is given by: \begin{align*} \boldsymbol{h}^{(t)} &= \sigma(\boldsymbol{Ux}^{(t)}+\boldsymbol{Wh}^{(t-1)}+\boldsymbol{b}) \tag{1} \end{align*}where the parameters $\boldsymbol{U}$, $\boldsymbol{W}$, $\boldsymbol{b}$ represent the input-to-hidden connections,hidden-to-hidden connections and bias vector respectively, and $\sigma$ is denoted as an activation function. Mechanism&emsp;As a convenience, we here will take the weight matrix $\boldsymbol{W}$ for example to analyze the exploding and vanishing gradients problems. Introduced in Deep Learning, variable $\boldsymbol{W}^{(t)}$, which is defined to be the copy of weight matrix $\boldsymbol{W}$ used only in time $t$, will be used in the computation of $\frac{\partial{L}}{\partial{\boldsymbol{W}}}$ by accumulating the factor $\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} $ through time. The computation of gradient $\frac{\partial{L}}{\partial{\boldsymbol{W}}}$ can be written in a sum-of-products form as follows: \begin{align*} \frac{\partial{L}}{\partial{\boldsymbol{W}}} &= \sum_{1 \le t \le \tau}\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} \tag{2}\\ \frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} &= \sum_{t \le k \le \tau}\frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}} \frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}} \frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}} \tag{3} \\ \frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}} &= \prod_{t \lt i \le k}\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}} = \prod_{t \lt i \le k}\boldsymbol{W}^{\mathsf{T}}\text{diag}(\sigma'(\boldsymbol{h}^{(i-1)})) \tag{4} \end{align*}where $\sigma’$ computes element-wise the derivative of $\sigma$.As the equation (3) shows, gradient term $\frac{\partial{L}}{\partial{\boldsymbol{W}^{(t)}}} $ consists of temporal components $\frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}}\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}}\frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}}$ that measures how $L$ at setp $k$ affects the gradient at step $t \lt k$. Notice that factor $\frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}}$ take the form of a product of $k-t$ Jacobian matrices (see eq. (4)), naturally, we’d think it behaves in the same way a product of $k-t$ real number $n$ shrink to zero ($0\lt n \lt 1$) or explode to infinity ($n \gt 1$) when $k-t$ approaches to infinity.To formalize these intuitions, in what follows a method proposed by R. Pascanu et al. (2012) will be used to obtain tight conditions for when the gradients explode or vanish.At first, let’s set $\sigma$ to the identity function in equation (1) so that the $\text{diag}(\sigma’(\boldsymbol{h}^{(i-1)}))$ will be an identity matrix $\boldsymbol{I}$. Consequently, equtaion (4) could be written in a product of $k-t$ matrices form: \frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}} = \prod_{t \lt i \le k}\boldsymbol{W}^{\mathsf{T}} = \left(\boldsymbol{W}^{\mathsf{T}} \right)^{(k-t)} \tag{5}, and $\boldsymbol{W}$ can be further diagonalized to better understand the $k-t$th power of $\boldsymbol{W}$ if the n by n matrix $\boldsymbol{W}$ has n linearly indepentent eigenvectors: \begin{align*} \boldsymbol{W} &= \boldsymbol{S}\boldsymbol{\Lambda}\boldsymbol{S}^{-1} \tag{6}\\ \left( \boldsymbol{W}^{\mathsf{T}} \right)^{(k-t)} &= \left( \boldsymbol{S}\boldsymbol{\Lambda}^{(k-t)}\boldsymbol{S}^{-1} \right)^{\mathsf{T}} \tag{7} \end{align*}where $\boldsymbol{\Lambda}$ is the eigenvalue matrix and $\boldsymbol{S}$ is eigenvector matrix. Induced by eq. (7), it’s sufficient for $0 \lt \rho \lt 1$, where the $\rho$ is the spectral radius of the weight matrix $\boldsymbol{W}$, for long term components to vanish (as $t \to \infty$) and necessary for $\rho \gt 1$ for them to blow up.Since the norm is used for matrices to measure the size of a matrix, we now generalize the reasult above for nonlinear functions $\sigma$ where $|\sigma’(x)|$ is bounded, $\Vert\text{diag}(\sigma’(\boldsymbol{x}))\Vert \le \gamma \in \mathcal{R}$, by relying on singular values.We suppose that it is sufficient for $\lambda_1 \lt \frac{1}{\gamma}$, where $\lambda_1$ is the largest singular value of $\boldsymbol{W}$, for the vanishing gradient problem. Note that Jocabian matrix $\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}}$ is given by $\boldsymbol{W}^{\mathsf{T}}\text{diag}(\sigma’(\boldsymbol{h}^{(i-1)}))$, we apply the triangle inequality of 2-norm of matrix to it: \forall i,\ \left\Vert\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}}\right\Vert \le \left\Vert\boldsymbol{W}^{\mathsf{T}}\right\Vert \left\Vert\text{diag}(\sigma'(\boldsymbol{h}^{(i-1)})) \right\Vert \lt \frac{1}{\gamma}\gamma = 1 \tag{8}Let $\eta \in \mathbb{R}$ be such that $\forall i, \left\Vert\frac{\partial{\boldsymbol{h}^{(i)}}}{\partial{\boldsymbol{h}^{(i-1)}}} \right\Vert \le \eta \lt 1$, where the existence of $\eta$ is given by eq. (8). With this supremum, we have: \left\Vert \frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}} \frac{\partial{\boldsymbol{h}^{(k)}}}{\partial{\boldsymbol{h}^{(t)}}} \frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}} \right\Vert \le \eta^{k-t} \left\Vert \frac{\partial{L}^{(k)}}{\partial{\boldsymbol{h}^{(k)}}} \right\Vert \left\Vert \frac{\partial{\boldsymbol{h}^{(t)}}}{\partial{\boldsymbol{W}^{(t)}}} \right\Vert \tag{9}As the equation above shown, the long term contribution will go to 0 exponentially fast as $t-k$ grows because of $\eta \lt 1$. Similarly, we can get the necessary condition for the largest singular value $\lambda_1 \gt \frac{1}{\gamma}$, for the exploding gradient problem. For activation function sigmoid we have $\gamma = \frac{1}{4}$, for tanh we have $\gamma = 1$. Solutions to the vanishing and exploding gradientScaling down the exploding gradientUsing L1 or L2 penalty on recurrent weight matrix is a good choice to deal with the exploding gradient problem. Compared with it, clipping the norm of matrix would be more simple and computationally efficient whenever it goes over a threshold although it introduce an additional hyper-parameter. Vanishing gradient regularizationTo address the vanishing gradient problem, Hochreiter and Schmidhuber (1997) propose the LSTM model that enforcing constant error flow through some special units. And R. Pascanu et al. (2012) presented a regularization term so that gradients neither increase or decrease in magnitude. But frankly speaking, I can’t catch the point what the regularization term excatly means in mathematics. ConclusionIn this post, I simply analyze the exploding and vanishing gradient problems in training RNNs by exploring the norm of gradients on weight matrices due to R. Pascanu et al. (2012). The problems seem to be an obstacle keeping people from retaining information from the long time lags. Hopefully, many approaches were suggested to deal with them. For instance, it is universally acknowledged that LSTM models are widely applied in a varity of tasks and lead to the incredible successes. So, I’d like to explore the LSTMs in next post.]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Computation of Gradients on RNN by BPTT]]></title>
    <url>%2F2018%2F11%2F08%2FBPTT%2F</url>
    <content type="text"><![CDATA[&emsp;As a beginner of deep learning, it’s quite easy to get lost when you attempt to compute the gradient on a recurrent neural network(RNN). Therefore, we will give the computation of gradients on RNN by backforward propagation through time(BPTT). I assume that you have known the concept of backward-propagation algorithm, recurrent neural networks(RNN), linear algebra and matrix calculus before the exploration. Notations&emsp;These notations used in the following computation refer to the book Deep Learning.$\begin{array}{l|l}\hlinea &amp; \text{Scalar a}\\\hline\boldsymbol{x} &amp; \text{Vector x} \\\hline\boldsymbol{1}_{n} &amp; \text{A vector [1,1,…,1] with n column}\\\hline\boldsymbol{A} &amp; \text{Matrix A}\\\hline\boldsymbol{A}^{-1} &amp; \text{The inverse matrix of matrix A}\\\hline\boldsymbol{A}^{\mathsf{T}} &amp; \text{Transpose of matrix A}\\\hline\text{diag}(\boldsymbol{a}) &amp; \text{Diagnal matrix with diagnal entries gieven by }\boldsymbol{a}\\\hline\boldsymbol{I} &amp; \text{Identity matrix}\\\hline\nabla_{\boldsymbol{x}}y &amp; \text{Gradient }y\text{ with respect to }\boldsymbol{x} \\\hline\frac{\partial{y}}{\partial{x}} &amp; \text{Partial derivative of } y \text{ with respect to }x\\\hline\frac{\partial{\boldsymbol{y}}}{\partial{\boldsymbol{x}}} &amp; \text{Jacobian matrix }\boldsymbol{J} \in \mathbb{R}^{m \times n} \text{of } f:\mathbb{R}^{n}\to\mathbb{R}^{m} \\\hline\end{array}$ Review of RNN Figure 2.1: The computational graph of RNN @Deep Leaerning &emsp;Let’s develop the forward propagation equations for the RNN dipcted in Figure 2.1. As a convenience, we use right superscription to indiacte the specified state of RNN at some time $ t $.To represent the hidden units of the network, we use the variable $ \boldsymbol{h} $, which will be the input of hyperbolic tangent activation function. As you see, the RNN has input-to-hidden connections parametrized by a weight matrix $ \boldsymbol{U} $, hidden-to-hidden connections parametrized by a weight matrix $ \boldsymbol{W} $, hidden-to-output connections parametrized by a weight matrix $ \boldsymbol{V} $. We can apply the softmax operation to the output $ \boldsymbol{o} $ to obtain a vector $ \boldsymbol{\hat{y}} $ of normalized probabilities over the output. We define the loss function as the negative log-likelihood of output vector $ \boldsymbol{\hat{y}} $ given the training target $ \boldsymbol{y} $. With notations and definitions made, we have the following equations: \left\{ \begin{aligned} \boldsymbol{a}^{(t)} &= \boldsymbol{b} + \boldsymbol{Wh}^{(t-1)} + \boldsymbol{Ux}^{(t)} &\qquad (2.1)\\ \boldsymbol{h}^{(t)} &= \text{tanh}(\boldsymbol{a}^{(t)}) &\qquad (2.2)\\ \boldsymbol{o}^{(t)} &= \boldsymbol{c} + \boldsymbol{Vh}^{(t)} &\qquad (2.3)\\ \boldsymbol{\hat{y}}^{(t)} &= \text{softmax}(\boldsymbol{o}^{(t)}) &\qquad (2.4)\\ L^{(t)} &= -{\boldsymbol{y}^{(t)}}^{\mathsf{T}}\text{log}(\boldsymbol{\hat{y}}^{(t)}) &\qquad (2.5) \end{aligned} \right.where both $ \text{tanh} $ and $\text{log}$ are element-wise function. Taking the Derivatives&emsp;Known the notations and concepts above, we can compute the gradients by BPTT using the identities below. Conditions Expression Denominator layout a = a(\boldsymbol{x}), \boldsymbol{u = u}(\boldsymbol{x}) \frac{\partial{a\boldsymbol{u}}}{\partial{\boldsymbol{x}}} a\frac{\partial{\boldsymbol{u}}}{\partial{\boldsymbol{x}}} + \frac{\partial{a}}{\partial{\boldsymbol{x}}}\boldsymbol{u}^{\mathsf{T}} $ \boldsymbol{A} $ is not a function of $ \boldsymbol{x} $ $ \frac{\partial{\boldsymbol{Ax}}}{\partial{\boldsymbol{x}}} $ $ \boldsymbol{A}^{\mathsf{T}} $ $g(\boldsymbol{U})\text{ is a scalar, }\boldsymbol{U} = \boldsymbol{U}(\boldsymbol{X}) $ $\frac{\partial{g(\boldsymbol{U})}}{\partial{\boldsymbol{X}}_{ij}} $ $\text{tr}\left(\left(\frac{\partial{g(\boldsymbol{U})}}{\partial{\boldsymbol{U}}}\right)^{\mathsf{T}}\frac{\partial{\boldsymbol{U}}}{\partial{\boldsymbol{X}}_{ij}}\right) $ we here will give three examples of computations in detail. Example 1To begin with, we will take the derivative of loss function $ L $ with respect to vector $ \boldsymbol{o} $. Using denominator-layout and chain rule, we have: \begin{aligned} \nabla_{\boldsymbol{o}^{(t)}}L &= \left( \frac{\partial{\boldsymbol{\hat{y}}}}{\partial{\boldsymbol{o}^{(t)}}}\frac{\partial{L}}{\partial{\boldsymbol{\hat{y}}}} \right) \end{aligned} \tag{3.1.1}Taking the former derivative of $ \boldsymbol{\hat{y}} $ with respect to $ \boldsymbol{o} $, we get: \begin{aligned} \frac {\partial{\boldsymbol{\hat{y}}}} {\partial{\boldsymbol{o}^{(t)}}} &= \frac {\partial{\frac{\text{exp}(\boldsymbol{o^{(t)}})}{\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}}} {\partial{\boldsymbol{o}^{(t)}}}\\ &= \frac {1} {\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})} \frac {\partial{\text{exp}(\boldsymbol{o}^{(t)})}} {\partial{\boldsymbol{o}^{(t)}}} + \frac {\partial{\frac {1} {\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}}} {\partial{\boldsymbol{o}^{(t)}}} \text{exp}(\boldsymbol{o}^{(t)})^{\mathsf{T}}\\ &= \frac {1} {\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})} \text{diag}\left( \text{exp}(\boldsymbol{o}^{(t)}) \right) -\left({\frac {1} {\boldsymbol{1_c}^{\mathsf{T}}\text{exp}(\boldsymbol{o}^{(t)})}} \right)^2 \text{exp}(\boldsymbol{o}^{(t)}) \text{exp}(\boldsymbol{o}^{(t)})^{\mathsf{T}}\\ &= \text{diag}(\boldsymbol{\hat{y}}) - \boldsymbol{\hat{y}}{\boldsymbol{\hat{y}}}^{\mathsf{T}} \end{aligned} \tag{3.1.2}For the latter partial derivative, we have: \begin{aligned} \frac {\partial{L}} {\partial{\boldsymbol{\hat{y}}}} &= \frac {\partial{-\boldsymbol{y}^{\mathsf{T}}\text{log}(\boldsymbol{\hat{y}})}} {\partial{\boldsymbol{\hat{y}}}}\\ &= \frac{\partial{\text{log}(\boldsymbol{\hat{y}})}}{\partial{\boldsymbol{\hat{y}}}} \frac{\partial{-\boldsymbol{y}^{\mathsf{T}}\text{log}(\boldsymbol{\hat{y}})}}{\partial{\text{log}(\boldsymbol{\hat{y}})}}\\ &= \text{diag}(\boldsymbol{\hat{y}})^{-1}(-\boldsymbol{y}) \end{aligned} \tag{3.1.3}Therefore, gradient $ \nabla_{\boldsymbol{o}^{(t)}}L $ at time step $ t $ is as follows: \begin{aligned} \nabla_{\boldsymbol{o}^{(t)}}L &= \frac{\partial{\boldsymbol{\hat{y}}}}{\partial{\boldsymbol{o}^{(t)}}}\frac{\partial{L}}{\partial{\boldsymbol{\hat{y}}}}\\ &= \left( \text{diag}(\boldsymbol{\hat{y}}) - \boldsymbol{\hat{y}}\boldsymbol{\hat{y}}^{\mathsf{T}} \right) \left( \text{diag}(\boldsymbol{\hat{y}})^{-1}(-\boldsymbol{y}) \right)\\ &= \left( \boldsymbol{I-\hat{y}}\boldsymbol{1_c}^{\mathsf{T}} \right)(-\boldsymbol{y})\\ &= \boldsymbol{\hat{y}-y} \end{aligned} \tag{3.1.4}where the training target $ \boldsymbol{y} $ is a basic vector [0,…,0,1,0,…,0] with a 1 at the postion $ i $. Example 2&emsp;Now, we consider a slightly more complicate example: computation of $\nabla_{\boldsymbol{h}^{(t)}}L$.Walking through computational graph backforward, it’s easy to get the gradient at the final time step $ \tau $ because the $\boldsymbol{h}^{(t)}$ has only a descendent $\boldsymbol{o}^{(t)}$: \nabla_{\boldsymbol{h}^{(t)}}L = \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L \tag{3.2.1}However, we should note that the $\boldsymbol{h}^{(t)}$ has both $\boldsymbol{o}^{(t)}$ and $\boldsymbol{h}^{(t+1)}$ as descendents from $ t = \tau - 1 $ down to $t = 1$. Consequently, the gradient is given by: \nabla_{\boldsymbol{h}^{(t)}}L = \left( \frac{\partial{\boldsymbol{h}^{(t+1)}}}{\partial{\boldsymbol{h}^{(t)}}} \right)^{\mathsf{T}}\nabla_{\boldsymbol{h}^{(t+1)}}L + \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L \tag{3.2.2}Taking the two cases above into account, we have: \begin{aligned} \nabla_{\boldsymbol{h}^{(t)}}L &= \begin{cases} \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad \text{if t at the final time step }\tau \\ \boldsymbol{W}^{\mathsf{T}}\text{diag}\left( 1-(\boldsymbol{h}^{(t+1)})^2 \right) (\nabla_{\boldsymbol{h}^{(t+1)}}L) + \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad 1\le t \le \tau - 1\\ \end{cases}\\ \end{aligned} \tag{3.2.3}Example 3&emsp;Once the gradients on internal nodes were computed, we can take the derivatives of the parameters, e.g. $ \boldsymbol{W} $, $ \boldsymbol{V} $. Notice that the parameters are shared across the time steps, we will introduce some dummy variables such as $\boldsymbol{W}^{(t)}$ that are define to be copies of parameter but with each only used at time step $t$, which will be accumulated from $ t = \tau $ down to $ t = 0 $ so that we can obtain the gradient $ \nabla_{\boldsymbol{W}}L $ at the end of a backward propagation.Applying the notations above, the gradient on the parameter $\nabla_{\boldsymbol{V}}L $ is given by: \begin{aligned} \nabla_{\boldsymbol{V}}L &= \sum_{t = 0}^{\tau}\nabla_{\boldsymbol{V}^{(t)}}L\\ &= \sum_{t = 0}^{\tau}\left[ \begin{matrix} \text{tr}\left( (\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{11}}} \right) &\cdots & \text{tr}\left( (\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{1n}}} \right)\\ \vdots &\ddots & \vdots\\ \text{tr}\left( (\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{n1}}} \right) &\cdots & \text{tr}\left( (\nabla_{\boldsymbol{o}^{(t)}}L)^{\mathsf{T}}\frac{\partial{\boldsymbol{o}^{(t)}}}{\partial{\boldsymbol{V}_{nn}}} \right) \end{matrix}\right] \\ &= \sum_{t = 0}^{\tau}\left[\begin{matrix} (\nabla_{\boldsymbol{o}^{(t)}}L)_{1}\boldsymbol{h}^{(t)}_{1} & \cdots & (\nabla_{\boldsymbol{o}^{(t)}}L)_{1}\boldsymbol{h}^{(t)}_{n}\\ \vdots &\ddots & \vdots\\ (\nabla_{\boldsymbol{o}^{(t)}}L)_{n}\boldsymbol{h}^{(t)}_{1} & \cdots & (\nabla_{\boldsymbol{o}^{(t)}}L)_{n}\boldsymbol{h}^{(t)}_{n} \end{matrix}\right] \\ &= \sum_{t = 0}^{\tau}(\nabla_{\boldsymbol{o}^{(t)}}L){\boldsymbol{h}^{(t)}}^{\mathsf{T}} \end{aligned} \tag{3.3.1}In fact, inspired by the relationship between total derivative and partial derivative in multivariable calculus: \text{d}f = \sum_{i=1}^{n}\frac{\partial{f}}{\partial{x_i}}\text{d}x_i \tag{3.3.2}which can be written in vector form as: \text{d}f = \left( \frac{\partial{f}}{\partial{\boldsymbol{x}}} \right)^{\mathsf{T}} \text{d}\boldsymbol{x} \tag{3.3.3}we can obtian the relationship between total derivative and partial derivative in matrix calcalus: \begin{aligned} \text{d}f &= \sum_{i=1}^{m}\sum_{j=1}^{n} \frac{\partial{f}}{\partial{\boldsymbol{X}_{ij}}}\text{d}\boldsymbol{X}_{ij}\\ &= \text{tr}\left( \left( \frac{\partial{f}}{\partial{\boldsymbol{X}}} \right)^{\mathsf{T}} \text{d}\boldsymbol{X} \right) \end{aligned} \tag{3.3.4}Now, we can skip the tedious process (3.3.1) by applying eq. (3.3.4) for gradient $ \nabla_{\boldsymbol{V}}L $: \begin{aligned} \text{d}L &= \text{tr}\left( \left( \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}} \right)^{\mathsf{T}} \text{d}\boldsymbol{o}^{(t)} \right)\\ &= \text{tr}\left( \left( \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}} \right)^{\mathsf{T}} \text{d}\left( \boldsymbol{c} + \boldsymbol{V}^{(t)}\boldsymbol{h}^{(t)} \right) \right)\\ &= \text{tr}\left( \left( \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}} \right)^{\mathsf{T}} \left( \left(\text{d} \boldsymbol{V}^{(t)} \right)\boldsymbol{h}^{(t)} + \boldsymbol{V}^{(t)}\text{d}\boldsymbol{h}^{(t)} \right) \right)\\ &= \text{tr}\left( \left( \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}} \right)^{\mathsf{T}} \left(\text{d} \boldsymbol{V}^{(t)} \right)\boldsymbol{h}^{(t)} \right)\\ &= \text{tr}\left( \boldsymbol{h}^{(t)} \left( \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}} \right)^{\mathsf{T}} \left(\text{d} \boldsymbol{V}^{(t)} \right) \right) \end{aligned} \tag{3.3.5}We could derive the result by comparing eq. (3.3.4) with eq. (3.3.5): \begin{aligned} \nabla_{\boldsymbol{V}^{(t)}}L &= \frac{\partial{L}}{\partial{\boldsymbol{V}^{(t)}}}\\ &= \left( \boldsymbol{h}^{(t)} \left( \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}} \right)^{\mathsf{T}} \right)^{\mathsf{T}}\\ &= \frac{\partial{L}}{\partial{\boldsymbol{o}^{(t)}}}{\boldsymbol{h}^{(t)}}^{\mathsf{T}}\\ &= \left( \nabla_{\boldsymbol{o}^{(t)}}L \right){\boldsymbol{h}^{(t)}}^{\mathsf{T}} \end{aligned} \tag{3.3.6} \begin{aligned} \nabla_{\boldsymbol{V}}L &= \sum_{i=0}^{\tau}\nabla_{\boldsymbol{V}^{(t)}}L\\ &= \sum_{i=0}^{\tau} \left( \nabla_{\boldsymbol{o}^{(t)}}L \right){\boldsymbol{h}^{(t)}}^{\mathsf{T}} \end{aligned} \tag{3.3.7}Similarly, using the equation (3.3.4), the gradient on remaining parameters is quite easy to get and we can gather these gradients as follows: \begin{align*} \nabla_{\boldsymbol{o}^{(t)}}L &= \boldsymbol{\hat{y}-y} \tag{3.3.8} \\ \nabla_{\boldsymbol{h}^{(t)}}L &= \begin{cases} \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad \text{if t at the final time step }\tau \\ \boldsymbol{W}^{\mathsf{T}}\text{diag}\left( 1-(\boldsymbol{h}^{(t+1)})^2 \right) (\nabla_{\boldsymbol{h}^{(t+1)}}L) + \boldsymbol{V}^{\mathsf{T}}\nabla_{\boldsymbol{o}^{(t)}}L, & \qquad 1\le t \le \tau - 1\\ \end{cases} \tag{3.3.9} \\ \nabla_{\boldsymbol{c}}L &= \sum_{t}\nabla_{\boldsymbol{o}^{(t)}}L \tag{3.3.10} \\ \nabla_{\boldsymbol{b}}L &= \sum_{t}\text{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\nabla_{\boldsymbol{h}^{(t)}}L \tag{3.3.11} \\ \nabla_{\boldsymbol{V}}L &= \sum_{t}\left( \nabla_{\boldsymbol{o}^{(t)}}L \right){\boldsymbol{h}^{(t)}}^{\mathsf{T}} \tag{3.3.12} \\ \nabla_{\boldsymbol{W}}L &= \sum_{t}\text{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t)}}L\right){\boldsymbol{h}^{(t-1)}}^{\mathsf{T}} \tag{3.3.13} \\ \nabla_{\boldsymbol{U}}L &= \sum_{t}\text{diag}\left(1-\left(\boldsymbol{h}^{(t)}\right)^{2}\right)\left(\nabla_{\boldsymbol{h}^{(t)}}L\right){\boldsymbol{x}^{(t)}}^{\mathsf{T}} \tag{3.3.14} \end{align*}]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>BPTT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[An Overview to LSTM]]></title>
    <url>%2F2018%2F10%2F23%2FLSTM%2F</url>
    <content type="text"><![CDATA[The Problem LSTM Addressed&emsp;In the past decades, recurrent neural networks (RNNs) have been successufully applied to a variety problems: speech recognition, language modeling, prediction, etc. RNNs can use their internal state to process sequences of inputs and persist information as shown below in Fig. 1.&emsp;However, the RNNs don’t seem to be able to learn of long term dependencies in the input/output sequences. It’s believed the reason for that problem is that error signals flowing backward in time tend to blow up or vanish(Hochreiter 1991). LSTM&emsp;Fortunately, LSTM address thisLearning to store information RNN &amp; IIR The similarity between RNN(Recurrent Neural Network) and IIR(Infinite Inpulse Response) suddenly hit me when I learned the concept about LSTM at first. I realized that it is similar between RNN and IIR when I first came to the RNN.In the following study, the idea hits me again when learning Adam gredient descent. As a user of Reddit said, it is a two FIR. I guess that I’m not the first person to conceive the idea of similarity between RNN and IIR. So I Google the key words “IIR LSTM”. Not surprisingly, there are several papers based on the similarity between the IIR and RNN appear in the search results list. In the most recent paper “Feedforward Sequential Memory Networks: A New Structure to Learn Long-term Dependency”, the authors proposed a novel network structure to model sequential signals like speech or language.]]></content>
      <categories>
        <category>Paper Notes</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Deep Learning</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
</search>
